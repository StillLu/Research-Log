
# SAE for LLM Interpretability

## References

1. **Title:** *Jumping Ahead: Improving Reconstruction
Fidelity with JumpReLU Sparse Autoencoders*  
   **Authors:** SSenthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Kramár and Neel Nanda 
   **Link:** [https://arxiv.org/pdf/2407.14435]


---

## Paper Summaries

### 1. *Explaining in Style: Training a GAN to Explain a Classifier in Style Space*

**Method:**  
The authors propose using Generative Adversarial Networks (GANs) to explain classifier decisions by operating in the style space of the data. The GAN learns to manipulate style features that influence the classifier's decision, providing a visual explanation of what the model is focusing on.

**Results:**  
The method demonstrates that style manipulation can highlight critical features influencing model predictions, offering an interpretable and intuitive understanding of complex classifiers.
